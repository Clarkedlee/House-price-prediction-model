{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House price prediction competion \n\n\n\n## Introduction\n\nThe goal of the project is to discover the relationships between some the aspects of a residential house and their value/sale price. The linear regression should be perfect for this kind of problem.  \n\n## Goalt and Mathtological\nBased on the existing studies, some factors are weigh more than others in contributing to the house price.However, there are about 78 perspective realtive to a house been record into the dataframe.  There will be a screen before regression. \n\nOpptional:\n\nMoreover, there will be a additional study on the result of the regression. The additional research aims to quantify the weight of each element that contributes to the price difference. This study will split the factor into two catalogues, the positive contribution and the negative contribution.","metadata":{}},{"cell_type":"markdown","source":"#### Note Book Enviroment\n\nLoad required liberay","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#** Data Processing module and mathmedic module\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns #the seaborn libery for graphical plotting\nimport matplotlib.pyplot as plt\n\n#** Machine Learning module \n#standardlization the data\nfrom sklearn.preprocessing import StandardScaler,PolynomialFeatures\n\n# pipeline function for standardlization\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n# Linear Regression module\nfrom sklearn.linear_model import LinearRegression \n\n# Ridge Regression module\nfrom sklearn.linear_model import Ridge\n\n#for model valuation \nfrom sklearn.model_selection import cross_val_score\n\n# for spreeding training and test data\nfrom sklearn.model_selection import train_test_split\n\n# Pickle liberay for serializing machine learning algorithm and save the serialzed format to a file\nimport pickle\n\n# #show all result in the output\n# from IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = \"all\"\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-20T07:02:04.131594Z","iopub.execute_input":"2023-02-20T07:02:04.132132Z","iopub.status.idle":"2023-02-20T07:02:04.144479Z","shell.execute_reply.started":"2023-02-20T07:02:04.132095Z","shell.execute_reply":"2023-02-20T07:02:04.142708Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ## The dataset of the esidential homes in Ames, Iowa.\n\nThe list of the features in the dataset and their explaination\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","metadata":{}},{"cell_type":"markdown","source":"#### Load datas into Notebook","metadata":{}},{"cell_type":"code","source":"#** Load dataset from source folder\nPARTH = \"/kaggle/input/house-prices-advanced-regression-techniques/\"\n\ntest_df = pd.read_csv(PARTH + '/test.csv')\n# test_price_df = pd.read_csv(PARTH + '/sample_submission.csv')\n\n\ntrain_df = pd.read_csv(PARTH + '/train.csv')\n\n# test_df.columns\n\ntrain_df.info()\n\n# test_df.info()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.150765Z","iopub.execute_input":"2023-02-20T07:02:04.151789Z","iopub.status.idle":"2023-02-20T07:02:04.240577Z","shell.execute_reply.started":"2023-02-20T07:02:04.151751Z","shell.execute_reply":"2023-02-20T07:02:04.239407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Statistic overview of the data set\ntrain_df.describe()\ntest_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.318747Z","iopub.execute_input":"2023-02-20T07:02:04.319283Z","iopub.status.idle":"2023-02-20T07:02:04.513625Z","shell.execute_reply.started":"2023-02-20T07:02:04.319239Z","shell.execute_reply":"2023-02-20T07:02:04.512100Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"LandContour\"].groupby(train_df[\"LandContour\"]).count()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.516278Z","iopub.execute_input":"2023-02-20T07:02:04.517342Z","iopub.status.idle":"2023-02-20T07:02:04.527968Z","shell.execute_reply.started":"2023-02-20T07:02:04.517293Z","shell.execute_reply":"2023-02-20T07:02:04.527010Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.set_index('Id')","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.529571Z","iopub.execute_input":"2023-02-20T07:02:04.530111Z","iopub.status.idle":"2023-02-20T07:02:04.566039Z","shell.execute_reply.started":"2023-02-20T07:02:04.530068Z","shell.execute_reply":"2023-02-20T07:02:04.565225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## House cleaning for the data\n\n### checking missing values","metadata":{}},{"cell_type":"code","source":"\n# Check how many columns in table\ncolumn_numbers = train_df.columns.size\n\nprint(\"There are\", column_numbers , \"columns in the table\")\n\n# Check if there is any empty columns \n\nnull_list = train_df.isnull().sum()\nnull_list.sort_values(ascending=False).head(20)\n\n#From the result,\n# there are 19 of the column has got no at least one null value\n#in them\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.568562Z","iopub.execute_input":"2023-02-20T07:02:04.568927Z","iopub.status.idle":"2023-02-20T07:02:04.584767Z","shell.execute_reply.started":"2023-02-20T07:02:04.568897Z","shell.execute_reply":"2023-02-20T07:02:04.583481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Eleminate the unwanted features\n\n\n#### 1. select by missing value and the importanest of the features\nBy Looking at the definition of the features map, and taking consideration on the number of data has been missing in each of the feature, some of features column will be removed. \n\nThe following columns will be remove from the dataset first:\n * PoolQC          \n * MiscFeature     \n * Alley           \n * Fence           \n * FireplaceQu      \n * LotFrontage      \n * GarageYrBlt       \n * GarageCond       \n * GarageType        \n * GarageFinish      \n * GarageQual        \n * BsmtFinType2      \n * BsmtExposure      \n * BsmtQual          \n * BsmtCond          \n * BsmtFinType1      \n * MasVnrArea         \n * MasVnrType\n","metadata":{}},{"cell_type":"code","source":"# Remove the unwanted column from the original table \n\n#from the previous operation, the name of the column with null vakue \n# have been extract by the sort_function. \n# Use this list to drop the columns\n\nunwanted_column = null_list.sort_values(ascending=False).head(18).index\n\n# This is the operation which used once and only.\ntrain_df.drop(unwanted_column, inplace=True,axis = 1)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.586613Z","iopub.execute_input":"2023-02-20T07:02:04.587476Z","iopub.status.idle":"2023-02-20T07:02:04.596195Z","shell.execute_reply.started":"2023-02-20T07:02:04.587415Z","shell.execute_reply":"2023-02-20T07:02:04.595004Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"There are\",train_df.columns.size, \"in the new table\" )\ntrain_df.set_index('Id')","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.662369Z","iopub.execute_input":"2023-02-20T07:02:04.663451Z","iopub.status.idle":"2023-02-20T07:02:04.699122Z","shell.execute_reply.started":"2023-02-20T07:02:04.663403Z","shell.execute_reply":"2023-02-20T07:02:04.697639Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Review dataset after first cleanup process\ntrain_df.info()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.703507Z","iopub.execute_input":"2023-02-20T07:02:04.703904Z","iopub.status.idle":"2023-02-20T07:02:04.728599Z","shell.execute_reply.started":"2023-02-20T07:02:04.703873Z","shell.execute_reply":"2023-02-20T07:02:04.727477Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inspect data features\n\nUse the function boxplot in the seaborn library to determine whether houses with a pool or without a pool have more price outliers.\n\n#### check the corelative rate between features and sale price","metadata":{}},{"cell_type":"code","source":"# check how close relative between the features and saleprice\n\ntrain_df.corr()['SalePrice'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.732478Z","iopub.execute_input":"2023-02-20T07:02:04.732902Z","iopub.status.idle":"2023-02-20T07:02:04.749822Z","shell.execute_reply.started":"2023-02-20T07:02:04.732869Z","shell.execute_reply":"2023-02-20T07:02:04.748365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(x=\"PoolArea\",y=\"SalePrice\",data=train_df)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:04.752293Z","iopub.execute_input":"2023-02-20T07:02:04.752624Z","iopub.status.idle":"2023-02-20T07:02:05.072463Z","shell.execute_reply.started":"2023-02-20T07:02:04.752594Z","shell.execute_reply":"2023-02-20T07:02:05.071241Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot showed that weather have pool or size of the pool is not close related to the price of house sold in Ames.\n\nUse regplot from seaborn to determine how the lot_area correlated with saleprice","metadata":{}},{"cell_type":"code","source":"sns.regplot(x=\"LotArea\",y=\"SalePrice\",data = train_df)\nplt.ylim()\nplt.xlim(0,)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:05.073898Z","iopub.execute_input":"2023-02-20T07:02:05.074259Z","iopub.status.idle":"2023-02-20T07:02:05.505178Z","shell.execute_reply.started":"2023-02-20T07:02:05.074227Z","shell.execute_reply":"2023-02-20T07:02:05.503934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Pandas methos corr() to find the features which are most correlated with the price\n\n#### 2.Select features by the richnest of the sample variation\n\nOne important consideration is to elimate the features which included a dominated feature. In some features in the dataset, it is obviously, there is a factors is dominated when compares with other factors in their own columns. \n\nFor example, in the column features (column) \"Street\". According to its definition, it descibe weather the street will be paved or not. However, there are very small amount streets were unpaved. Therefore, it isn't sinificant value in the regretion analysis.This feature will be removed.\n","metadata":{}},{"cell_type":"code","source":"test = train_df.groupby(\"Street\").count()\nsum(test[\"Id\"])\n\n# for i in test[\"Id\"]:\n    \ntest[\"Id\"] / sum(test[\"Id\"]) * 100\ntest.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:05.506697Z","iopub.execute_input":"2023-02-20T07:02:05.507468Z","iopub.status.idle":"2023-02-20T07:02:05.526422Z","shell.execute_reply.started":"2023-02-20T07:02:05.507432Z","shell.execute_reply":"2023-02-20T07:02:05.525294Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the same logic, we will go through all the features in the dataset, and elimate the features which have not enough variation. ","metadata":{}},{"cell_type":"code","source":"test_1 = train_df.groupby(\"Utilities\").count()\nsum(test_1[\"Id\"])\ntest_1[\"Id\"] / sum(test_1[\"Id\"]) * 100\ntest_1.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:05.527797Z","iopub.execute_input":"2023-02-20T07:02:05.528270Z","iopub.status.idle":"2023-02-20T07:02:05.547277Z","shell.execute_reply.started":"2023-02-20T07:02:05.528232Z","shell.execute_reply":"2023-02-20T07:02:05.545934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#*** Screen the feature maps for useful features\n\n#** Create a list to record the column which will be use in further analysis\n\ntrain_featured_list =  []\n\n# Use for loop to go over all the column \n# and look at the richnest of the variation\nfor x in range(61):\n    \n    #* Grouped the data by their features\n    test_1 = train_df.groupby(train_df.columns[x + 1]).count()\n    \n    #* calcualte the weight of each items in the features.\n    test_1[\"Id\"] / sum(test_1[\"Id\"]) * 100\n    \n    #* check the shape of the results in their table\n    test_1.shape\n\n    #check the total size of the results\n    size_result = test_1.size\n    size_result\n    #add the name of features(columns) to the train_featured_list\n    if size_result > 600: \n        train_featured_list.append(train_df.columns[x + 1]) \n    \n    #*** OR deteled the column from the dataset directly\n    # if size_result < 600:\n    #    train_df.drop(train_df.columns[x + 1])","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:05.550190Z","iopub.execute_input":"2023-02-20T07:02:05.550554Z","iopub.status.idle":"2023-02-20T07:02:05.977845Z","shell.execute_reply.started":"2023-02-20T07:02:05.550525Z","shell.execute_reply":"2023-02-20T07:02:05.976809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_featured_list\nlen(train_featured_list)\n\nfor x in range(25):\n    \n    #* Grouped the data by their features\n    test_1 = train_df.groupby(train_featured_list[x]).count()\n    \n    #* calcualte the weight of each items in the features.\n    test_1[\"Id\"] / sum(test_1[\"Id\"]) * 100\n    \n    size_result = test_1.size\n    size_result","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:05.979314Z","iopub.execute_input":"2023-02-20T07:02:05.979667Z","iopub.status.idle":"2023-02-20T07:02:06.189546Z","shell.execute_reply.started":"2023-02-20T07:02:05.979636Z","shell.execute_reply":"2023-02-20T07:02:06.188363Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of the column has been reduced to 26, from 81. However there are still bit too heavy for us to carryout, multiple value regradtion. Hence, the analysis will carry out to see if there are some other features which is less important to the analysis.\n\nBy revisting the columns left in the dataset, it is very obviously there are few more columns which contained some dominated factor. This columns can be get ripped off. ","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThis is a test code, to see  percent of a factor in\nthe feature column\n\"\"\"\ntest_2 = train_df.groupby(\"BsmtFinSF2\").count()\nresult_2 = test_2[\"Id\"] / sum(test_2[\"Id\"]) * 100\nresult_2.max()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.190943Z","iopub.execute_input":"2023-02-20T07:02:06.191348Z","iopub.status.idle":"2023-02-20T07:02:06.209502Z","shell.execute_reply.started":"2023-02-20T07:02:06.191313Z","shell.execute_reply":"2023-02-20T07:02:06.208302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Save a list of feature reminded\n\nThis is a list of feature after the second screen process.","metadata":{}},{"cell_type":"code","source":"train_featured_list_2 =  []\n\n# Use for loop to go over all the column from the previous analysis\n# and look at the which features still include a dominated factor\nfor x in train_featured_list:\n    \n    #* Grouped the data by their features\n    test_2 = train_df.groupby(x).count()\n    \n    #* calcualte the weight of each items in the features.\n    result_2 = test_2[\"Id\"] / sum(test_1[\"Id\"]) * 100\n    \n    #* check the shape of the results in their table\n    test_2.shape\n    result_2\n\n\n    #add the name of features(columns) to the train_featured_list\n    #if there aren't any factor has more than 50%. \n  \n    if (result_2.max() < 50): \n        train_featured_list_2.append(x) ","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.210777Z","iopub.execute_input":"2023-02-20T07:02:06.211130Z","iopub.status.idle":"2023-02-20T07:02:06.397586Z","shell.execute_reply.started":"2023-02-20T07:02:06.211099Z","shell.execute_reply":"2023-02-20T07:02:06.396356Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#show the result of the final list\n\"\"\" \nActually the selected feature_list is not neccesary, if the column\ndropped out dirctly during the process. But it is easy to track the\nchange of the list during the screening process with this method.\n\"\"\"\n\nlen(train_featured_list_2)\ntrain_featured_list_2\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.399200Z","iopub.execute_input":"2023-02-20T07:02:06.399581Z","iopub.status.idle":"2023-02-20T07:02:06.412371Z","shell.execute_reply.started":"2023-02-20T07:02:06.399550Z","shell.execute_reply":"2023-02-20T07:02:06.411289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chosen useful data columns\n\n### Feature selection consideration\n\nAfter the few screen processes, only 17 features (columns) are left for further analysis. All these features had been examined against to their definition, in order to find out wether they are essential factors for ordinary house buyers to consider when making a buying decision.\n\nHowever, a basement is not common to most of the country. In that case, the features related to the basement will be dropped out from the analysis. For example, BsmtFinSF1', 'BsmtUnfSF','TotalBsmtSF' etc..\n\nMoreover, in the feature list, there is an item called 'OpenPorchSF'. According to its definition, it is the open porch area in square feet. However, there are a few more features also related to the porch, which are:\n\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n\nThere is another feature called 'OpenPorchSF', which is related to 'Porch'. These features related to a \"Porch\"  are not common features  For simplicity purposes, they have been dropout in the screening process. \n\nIn the previous process, the Lot Area examined its co-relationship with Saleprice. The result showed that the lot area was positively related to the sale price. However, the weight of this particular is not significant for this regression. So it will be dropped too.\n\nAt last, the feature 'MoSold' will be removed as it is only around 0.04 correlated with the Price.","metadata":{}},{"cell_type":"markdown","source":"\nFinal list of feature for the regression:\n * 'MSSubClass'\n * 'Neighborhood'\n * 'OverallQual'\n * 'YearBuilt'\n * 'YearRemodAdd'\n * 'Exterior1st'\n * 'Exterior2nd'\n * '1stFlrSF'\n * 'GrLivArea'\n * 'TotRmsAbvGrd'\n * 'GarageArea'\n\n","metadata":{}},{"cell_type":"code","source":"#******* Created a final list for regresion without Basement factor\ntrain_festure_list_final = [\n'MSSubClass',\n'Neighborhood',\n'OverallQual',\n'YearBuilt',\n'YearRemodAdd',\n'Exterior1st',\n'Exterior2nd',\n'1stFlrSF',\n'GrLivArea',\n'TotRmsAbvGrd',\n'GarageArea'\n]\n\nlen(train_festure_list_final)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.415302Z","iopub.execute_input":"2023-02-20T07:02:06.416198Z","iopub.status.idle":"2023-02-20T07:02:06.427586Z","shell.execute_reply.started":"2023-02-20T07:02:06.416123Z","shell.execute_reply":"2023-02-20T07:02:06.426400Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test1 = train_df[\"Exterior1st\"].groupby(train_df[\"Exterior1st\"]).count()\n\ntest1/sum(test1) * 100","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.428924Z","iopub.execute_input":"2023-02-20T07:02:06.429308Z","iopub.status.idle":"2023-02-20T07:02:06.441797Z","shell.execute_reply.started":"2023-02-20T07:02:06.429277Z","shell.execute_reply":"2023-02-20T07:02:06.440561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the test data set, compare the training data \n# and the test data in feature Exterior1st\ntest2 = test_df[\"Exterior1st\"].groupby(test_df[\"Exterior1st\"]).count()\na1 = test_df[\"Exterior1st\"].groupby(test_df[\"Exterior1st\"]).count().index\na2 = train_df[\"Exterior1st\"].groupby(train_df[\"Exterior1st\"]).count().index\nmap(a1,a2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.443649Z","iopub.execute_input":"2023-02-20T07:02:06.444018Z","iopub.status.idle":"2023-02-20T07:02:06.457416Z","shell.execute_reply.started":"2023-02-20T07:02:06.443988Z","shell.execute_reply":"2023-02-20T07:02:06.456203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Represent ordinal\n\nIn the dataframe, features with categorical data will be into impossible to use for regresssion. Thus, all similar data will be reprocess and convert to Arbitary numbers. \n\nThe first categorical data need to be convert is Neighborhood. The Neighborhood should positive related to the saleprice due to the good nieghborhood is alway more popular then the not that good one. To determine the rank of the neighbourhood, the average price would be a good indication. ","metadata":{}},{"cell_type":"code","source":"# code move down to later process\n# Key_train_data,train_price = train_df[train_festure_list_final],train_df['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.458911Z","iopub.execute_input":"2023-02-20T07:02:06.459271Z","iopub.status.idle":"2023-02-20T07:02:06.464532Z","shell.execute_reply.started":"2023-02-20T07:02:06.459241Z","shell.execute_reply":"2023-02-20T07:02:06.463307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #*** Calculate the mean Saleprice for each \n# mean_price_by_neigh = train_df[['Neighborhood','SalePrice']].groupby(train_df.Neighborhood).mean('SalePrice')\n# # Key_train_data.Neighborhood.groupby(Key_train_data.Neighborhood).count()\n# Neig_rank_order = mean_price_by_neigh.sort_values(by = ['SalePrice'], ascending = False).index\n\n'''\nmean_price_by_neigh = train_df[['Neighborhood','SalePrice']].groupby(train_df.Neighborhood).mean('SalePrice')\n\nNeig_rank_order = mean_price_by_neigh.sort_values(by = ['SalePrice'], ascending = False).index\n'''\n\n\nNeig_rank_order = train_df[['Neighborhood','SalePrice']].groupby(train_df.Neighborhood).mean('SalePrice').rank(ascending = False).sort_values(\"SalePrice\").index\n\n# Convert Text data to ordinal training data\n#set object to category type\ntrain_df['Neighborhood'] = train_df['Neighborhood'].astype('category')\ntrain_df['Neighborhood'] = train_df['Neighborhood'].cat.reorder_categories(Neig_rank_order, ordered=True)\ntrain_df['Neighborhood'] = train_df['Neighborhood'].cat.codes\n\n# Do the same operation to test data frame\n\ntest_df['Neighborhood'] = test_df['Neighborhood'].astype('category')\ntest_df['Neighborhood'] = test_df['Neighborhood'].cat.reorder_categories(Neig_rank_order, ordered=True)\ntest_df['Neighborhood'] = test_df['Neighborhood'].cat.codes\n\nprint(train_df['Neighborhood'])\n\nprint(test_df['Neighborhood'])","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.466387Z","iopub.execute_input":"2023-02-20T07:02:06.466740Z","iopub.status.idle":"2023-02-20T07:02:06.492888Z","shell.execute_reply.started":"2023-02-20T07:02:06.466709Z","shell.execute_reply":"2023-02-20T07:02:06.491524Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Because there was a problem with encoding the exterior1st data into numerical data, the exterior1 will dropped.","metadata":{}},{"cell_type":"code","source":"# mean_price_by_mat = train_df[['Exterior1st','SalePrice']].groupby(train_df.Exterior1st).mean('SalePrice')\n\n# Mat_sp_order = mean_price_by_mat.sort_values(by = ['SalePrice'], ascending = False).index\n\n'''\nmean_price_by_mat = train_df[['Exterior1st','SalePrice']].groupby(train_df.Exterior1st).mean('SalePrice')\n\nMat_sp_order = mean_price_by_mat.sort_values(by = ['SalePrice'], ascending = False).index\n'''\n\n\nMat_sp_order = train_df[['Exterior1st','SalePrice']].groupby(train_df.Exterior1st).mean('SalePrice').rank(ascending = False).sort_values(\"SalePrice\").index\n\n\n\n    \n#     test_df['Exterior1st'].replace(to_replace = x, value = 'Python', inplace=True)\n\n# Convert Text data to ordinal data\n#set object to category type\ntrain_df['Exterior1st'] = train_df['Exterior1st'].astype('category')\ntrain_df['Exterior1st'] = train_df['Exterior1st'].cat.reorder_categories(Mat_sp_order, ordered=True)\ntrain_df['Exterior1st'] = train_df['Exterior1st'].cat.codes\n\n# Because there are different number categories between train_df and test_df in column Exterior1st.\n# Hence the following method is used to convert category data to numerical data.\nfor x in range(len(Mat_sp_order)):\n    test_df['Exterior1st'].replace(to_replace = Mat_sp_order[x], value = x, inplace=True)\n      \n\ntest_df['Exterior1st']","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.494809Z","iopub.execute_input":"2023-02-20T07:02:06.495219Z","iopub.status.idle":"2023-02-20T07:02:06.524567Z","shell.execute_reply.started":"2023-02-20T07:02:06.495179Z","shell.execute_reply":"2023-02-20T07:02:06.523132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_price_by_mat = train_df[['Exterior2nd','SalePrice']].groupby(train_df.Exterior2nd).mean('SalePrice')\n# Key_train_data.Neighborhood.groupby(Key_train_data.Neighborhood).count()\nmat_sp_order_2 = mean_price_by_mat.sort_values(by = ['SalePrice'], ascending = False).index\n\ntrain_df['Exterior2nd'] = train_df['Exterior2nd'].astype('category')\ntrain_df['Exterior2nd'] = train_df['Exterior2nd'].cat.reorder_categories(mat_sp_order_2, ordered=True)\ntrain_df['Exterior2nd'] = train_df['Exterior2nd'].cat.codes\n\nprint(train_df['Exterior2nd'])\n''' \n# test_df['Exterior2nd'] = test_df['Exterior2nd'].astype('category')\n# test_df['Exterior2nd'] = test_df['Exterior2nd'].cat.reorder_categories(mat_sp_order_2, ordered=True)\n# test_df['Exterior2nd'] = test_df['Exterior2nd'].cat.codes\n\nError Msg:\nitems in new_categories are not the same as in old categories\n'''\n\n\n\n# Because there are different number categories between train_df and test_df in column Exterior2st.\n# Hence the following method is used to convert category data to numerical data.\nfor x in range(len(mat_sp_order_2)):\n    test_df['Exterior2nd'].replace(to_replace = mat_sp_order_2[x], value = x, inplace=True)\n      \n\ntest_df['Exterior2nd']","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.531312Z","iopub.execute_input":"2023-02-20T07:02:06.532023Z","iopub.status.idle":"2023-02-20T07:02:06.565348Z","shell.execute_reply.started":"2023-02-20T07:02:06.531972Z","shell.execute_reply":"2023-02-20T07:02:06.563980Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Make a final dataset for the regrestion\n\nThe previous processes included detemine the features which use in the final regression and produced a list of the remaining feature; convert the category data into represented numerical data. The final dataset Key_train_data will be made from there.\n","metadata":{}},{"cell_type":"code","source":"# Create the training dataframe for final regretion\n\nKey_train_data,train_price = train_df[train_festure_list_final],train_df['SalePrice']\n# Key_train_data.head()\nKey_train_data.head()\nKey_train_data.info()\n\n#Create the test dataframe \n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.569846Z","iopub.execute_input":"2023-02-20T07:02:06.570704Z","iopub.status.idle":"2023-02-20T07:02:06.586979Z","shell.execute_reply.started":"2023-02-20T07:02:06.570652Z","shell.execute_reply":"2023-02-20T07:02:06.585683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing data frame\n\nKey_test_data = test_df[train_festure_list_final]\nKey_test_data.info()\n\n#the result showed there some missing numbers in the GarageArea, Exterior1st and Exterior2nd clomun\n# Used the filled null function to fill up the gap with the mean of GarageArea\nKey_test_data['GarageArea'].fillna(Key_test_data['GarageArea'].mean(), inplace = True)\nKey_test_data.info()\n# Key_test_data.head()\n\nKey_test_data['Exterior1st'].fillna(Key_test_data['Exterior1st'].mean(), inplace = True)\nKey_test_data.info()\n\nKey_test_data['Exterior2nd'].fillna(Key_test_data['Exterior2nd'].mean(), inplace = True)\nKey_test_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.591267Z","iopub.execute_input":"2023-02-20T07:02:06.591676Z","iopub.status.idle":"2023-02-20T07:02:06.638916Z","shell.execute_reply.started":"2023-02-20T07:02:06.591643Z","shell.execute_reply":"2023-02-20T07:02:06.637695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Module Development \n\n### Linear regression with single input","metadata":{}},{"cell_type":"code","source":"#Single feature linear regression\nX = train_df[[\"OverallQual\"]]\nY = train_df[\"SalePrice\"]\nY\nlinear_mod = LinearRegression()\nlinear_mod.fit(X,Y)\nlinear_mod.score(X,Y)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.640702Z","iopub.execute_input":"2023-02-20T07:02:06.641879Z","iopub.status.idle":"2023-02-20T07:02:06.656404Z","shell.execute_reply.started":"2023-02-20T07:02:06.641837Z","shell.execute_reply":"2023-02-20T07:02:06.655222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Linear regression with multiple inputs","metadata":{}},{"cell_type":"code","source":"linear_mod_1 = LinearRegression()\nZ = Key_train_data\nY1 = train_df[\"SalePrice\"]\nlinear_mod_1.fit(Z,Y1)\nYhat_1 = linear_mod_1.predict(Z)\nYhat_1[0:5]\nprint(\"The R^2 of the list Z:\", linear_mod_1.score(Z,Y1))","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.657790Z","iopub.execute_input":"2023-02-20T07:02:06.658178Z","iopub.status.idle":"2023-02-20T07:02:06.683578Z","shell.execute_reply.started":"2023-02-20T07:02:06.658107Z","shell.execute_reply":"2023-02-20T07:02:06.681331Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Add standardlization and polynomial transform extraction to the data set before regression. This processes goes through a pipeline procedure.","metadata":{}},{"cell_type":"code","source":"Input = [('scale',StandardScaler()),('polynomial', PolynomialFeatures(include_bias=False)),('model',LinearRegression())]","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.685516Z","iopub.execute_input":"2023-02-20T07:02:06.686186Z","iopub.status.idle":"2023-02-20T07:02:06.700502Z","shell.execute_reply.started":"2023-02-20T07:02:06.686108Z","shell.execute_reply":"2023-02-20T07:02:06.698477Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = Pipeline(Input)\npipe.fit(Z,Y1)\nypipe = pipe.predict(Z)\nypipe[0:5]\nprint(\"The R^2 of the list Z with Pipeline:\", pipe.score(Z,Y))\nypipe[0:5]","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.704276Z","iopub.execute_input":"2023-02-20T07:02:06.706778Z","iopub.status.idle":"2023-02-20T07:02:06.814187Z","shell.execute_reply.started":"2023-02-20T07:02:06.706726Z","shell.execute_reply":"2023-02-20T07:02:06.812227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ridge Regression\n\n#### Split the data\n\nThis the refinement of the regression. Split the training data further into training data and test data for varification","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split( Key_train_data, train_price, test_size = 0.10, random_state = 1)\nprint(\" Number of the Test samples:\", x_test.shape[0])\n\nprint(\" Number of the Train samples:\", x_train.shape[0])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.818002Z","iopub.execute_input":"2023-02-20T07:02:06.819362Z","iopub.status.idle":"2023-02-20T07:02:06.842805Z","shell.execute_reply.started":"2023-02-20T07:02:06.819313Z","shell.execute_reply":"2023-02-20T07:02:06.841345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Ridge regression modul, and set regulation parameter to 0.1\n\nridge_mod = Ridge(alpha = 0.1)\n#ridge_mod = make_pipeline(StandardScaler(),LinearRegression())\n# Fit the module\nridge_mod.fit(x_train,y_train)\n\n# print(\"Coefficients:\", ridge_mod.score() )\n\n#use model to predict the price\nyhat_2 = ridge_mod.predict(x_test)\nprint(\"Predicted Price:\" , yhat_2[0:10] )\nprint(\"Sample Price:\" , y_test[0:10] )\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.845963Z","iopub.execute_input":"2023-02-20T07:02:06.847847Z","iopub.status.idle":"2023-02-20T07:02:06.891917Z","shell.execute_reply.started":"2023-02-20T07:02:06.847788Z","shell.execute_reply":"2023-02-20T07:02:06.890666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the R square value for the rigid model\nprint(\"The R^2 \", ridge_mod.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.894027Z","iopub.execute_input":"2023-02-20T07:02:06.894881Z","iopub.status.idle":"2023-02-20T07:02:06.911052Z","shell.execute_reply.started":"2023-02-20T07:02:06.894835Z","shell.execute_reply":"2023-02-20T07:02:06.909209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Perform a second order polynomial transform on both the training data and testing data.","metadata":{}},{"cell_type":"code","source":"# Polynomial transform on training and test data\np_t = PolynomialFeatures(degree = 2)\nx_pt_train = p_t.fit_transform(x_train)\nx_pt_test  = p_t.fit_transform(x_test)\n\n# Rigid Regression Model\nrigid_mod_1 = Ridge(alpha = 0.1)\nrigid_mod_1.fit(x_pt_train, y_train)\ny_hat_2 = rigid_mod_1.predict(x_pt_test)\n\nprint(\"Predicted\",y_hat_2[0:10])\nprint(\"Train\",y_test[0:10])\n\nprint(\"The R^2 \", rigid_mod_1.score(x_pt_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.916578Z","iopub.execute_input":"2023-02-20T07:02:06.918572Z","iopub.status.idle":"2023-02-20T07:02:06.965724Z","shell.execute_reply.started":"2023-02-20T07:02:06.918497Z","shell.execute_reply":"2023-02-20T07:02:06.964035Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Save and load trained model\n\n# Save the model to disk\n# filename = \"CLK_trained_model.sav\"\n\n# pickle.dump(rigid_mod_1, open(filename,'wb'))\n\n\n#load the model trained\n# loaded_model = pickle.load(open(filename,\"rb\"))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.968724Z","iopub.execute_input":"2023-02-20T07:02:06.970001Z","iopub.status.idle":"2023-02-20T07:02:06.978715Z","shell.execute_reply.started":"2023-02-20T07:02:06.969930Z","shell.execute_reply":"2023-02-20T07:02:06.976424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final prediction \n\nUsing the best trained modual, rigid_mod_1 to predict the price of the house from the test data, and output the result to CSV files.","metadata":{}},{"cell_type":"code","source":"# Extra features from the test dataset\n# the Key_test_data has been refine and filled up the NaN in previous process\nX_final = Key_test_data\n\n# Polynomkial transform on the data\nx_pt_final = p_t.fit_transform(X_final)\n\n\ny_hat_5 = rigid_mod_1.predict(x_pt_final)\n\ny_hat_5[0:10]","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:06.984256Z","iopub.execute_input":"2023-02-20T07:02:06.986572Z","iopub.status.idle":"2023-02-20T07:02:07.022899Z","shell.execute_reply.started":"2023-02-20T07:02:06.986489Z","shell.execute_reply":"2023-02-20T07:02:07.021226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Output predicting result to CSV\ndata_id = test_df.Id\n\nfinal_predicion = pd.DataFrame({\"Id\"  : data_id.values,\n                                   \"SalePrice\": y_hat_5\n                                   }).set_index('Id')\nfinal_predicion\nfinal_predicion_csv = final_predicion.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:07.025193Z","iopub.execute_input":"2023-02-20T07:02:07.026354Z","iopub.status.idle":"2023-02-20T07:02:07.068083Z","shell.execute_reply.started":"2023-02-20T07:02:07.026282Z","shell.execute_reply":"2023-02-20T07:02:07.065520Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npd.read_csv(\"submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T07:02:07.077408Z","iopub.execute_input":"2023-02-20T07:02:07.078702Z","iopub.status.idle":"2023-02-20T07:02:07.108485Z","shell.execute_reply.started":"2023-02-20T07:02:07.078655Z","shell.execute_reply":"2023-02-20T07:02:07.107175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}